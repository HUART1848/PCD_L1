{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"Logo HEIG-VD\" style=\"width: 80px;\" align=\"right\"/>\n",
    "\n",
    "# Cours PCD - Labo 1\n",
    "\n",
    "# Extraction d'attributs pour la détection du spam\n",
    "\n",
    "## Résumé\n",
    "\n",
    "Le but de ce travail est de réaliser des expériences de classification d'emails en *spam* ou *ham* (c'est-à-dire non-spam) grâce aux données et au code fourni intégralement dans ce notebook Jupyter.\n",
    "\n",
    "Votre objectif sera de chercher les meilleurs attributs (*features*) qu'on puisse extraire des emails, ainsi que les classifieurs, afin de maximiser le score F1 de la classification spam/non-spam.  Les classifieurs utilisés seront de la famille Naive Bayes de Scikit-learn.\n",
    "\n",
    "Veuillez rendre un notebook avec votre meilleure stratégie.  Présentez clairement les options et leurs résultats, et commentez-les brièvement.\n",
    "\n",
    "*Source* : l'idée du labo vient du [premier notebook de Gan Sie Huai](https://github.com/huai99/Email-Spam-Detection-Python) avec plusieurs changements et mises à jour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtention des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez télécharger l'archive d'emails depuis Cyberlearn.  Ou téléchargez les cinq archives commençant par `20030228_` depuis https://spamassassin.apache.org/old/publiccorpus/.  Décompressez-les dans un dossier de votre choix dont vous indiquerez le chemin relatif dans `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/spam_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_ham_paths = glob.glob(path+'easy_ham/*')\n",
    "easy_ham_2_paths = glob.glob(path+'easy_ham_2/*')\n",
    "hard_ham_paths = glob.glob(path+'hard_ham/*')\n",
    "spam_paths = glob.glob(path+'spam/*')\n",
    "spam_2_paths = glob.glob(path+'spam_2/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_content(email_path):\n",
    "    file = open(email_path,encoding='latin1')\n",
    "    try:\n",
    "        msg = email.message_from_file(file)\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                return part.get_payload() # prints the raw text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "def get_email_content_bulk(email_paths):\n",
    "    email_contents = [get_email_content(o) for o in email_paths]\n",
    "    return email_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Division en *train* et *test*\n",
    "Le code suivant vous permet de diviser les données en *train* et *test* (sous-ensembles d'entraînement et de test).  Veuillez l'exécuter pour créer ces deux ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_path = [easy_ham_paths, easy_ham_2_paths, hard_ham_paths]\n",
    "spam_path = [spam_paths, spam_2_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_sample = np.array([train_test_split(o) for o in ham_path], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in [train_test_split(o) for o in ham_path]:\n",
    "    print(len(o[0]), len(o[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_train = np.array([])\n",
    "ham_test = np.array([])\n",
    "for o in ham_sample:\n",
    "    ham_train = np.concatenate((ham_train, o[0]), axis=0)\n",
    "    ham_test  = np.concatenate((ham_test,  o[1]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_train.shape, ham_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sample = np.array([train_test_split(o) for o in spam_path], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in [train_test_split(o) for o in spam_path]:\n",
    "    print(len(o[0]), len(o[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_train = np.array([])\n",
    "spam_test = np.array([])\n",
    "for o in spam_sample:\n",
    "    spam_train = np.concatenate((spam_train,o[0]),axis=0)\n",
    "    spam_test = np.concatenate((spam_test,o[1]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_train.shape, spam_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_train_label = [0]*ham_train.shape[0]\n",
    "spam_train_label = [1]*spam_train.shape[0]\n",
    "x_train = np.concatenate((ham_train,spam_train))\n",
    "y_train = np.concatenate((ham_train_label,spam_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_test_label = [0]*ham_test.shape[0]\n",
    "spam_test_label = [1]*spam_test.shape[0]\n",
    "x_test = np.concatenate((ham_test,spam_test))\n",
    "y_test = np.concatenate((ham_test_label,spam_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shuffle_index = np.random.permutation(np.arange(0,x_train.shape[0]))\n",
    "test_shuffle_index = np.random.permutation(np.arange(0,x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[train_shuffle_index]\n",
    "y_train = y_train[train_shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[test_shuffle_index]\n",
    "y_test = y_test[test_shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_email_content_bulk(x_train)\n",
    "x_test = get_email_content_bulk(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null(datas,labels):\n",
    "    not_null_idx = [i for i,o in enumerate(datas) if o is not None]\n",
    "    return np.array(datas)[not_null_idx],np.array(labels)[not_null_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = remove_null(x_train,y_train)\n",
    "x_test,y_test = remove_null(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question :** combien d'emails y a-t-il dans le jeu d'entraînement et dans le jeu de test ?  Combien de spam et de ham, respectivement ?  Veuillez écrire le code qui fournit les réponses ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pré-traitement et nettoyage des données\n",
    "\n",
    "### 3.1 Traitements de bas niveau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlink(word):\n",
    "    return  re.sub(r\"http\\S+\", \"\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(word):\n",
    "    result = word.lower()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_number(word):\n",
    "    result = re.sub(r'\\d+', '', word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(word):\n",
    "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(word):\n",
    "    result = word.strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_newline(word):\n",
    "    return word.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Votre mission :** dans la fonction suivante, insérer dans le tableau `cleaning_utils` les noms des fonctions de pré-traitement que vous souhaitez utiliser afin d'obtenir le meilleur filtre anti-spam.   Veuillez commenter brièvement les raisons de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_pipeline(sentence):\n",
    "    cleaning_utils = []\n",
    "    for o in cleaning_utils:\n",
    "        sentence = o(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [clean_up_pipeline(o) for o in x_train]\n",
    "x_test = [clean_up_pipeline(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Traitements linguistiques avec NLTK\n",
    "\n",
    "Veuillez installer la librairie NLTK avec Conda ou `pip`, voir instructions à https://www.nltk.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_stemmer(words):\n",
    "    return [stemmer.stem(o) for o in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lemmatizer(words):\n",
    "    return [lemmatizer.lemmatize(o) for o in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Votre mission :** dans la fonction suivante, insérer dans le tableau `cleaning_utils` les noms des fonctions de traitement linguistique que vous souhaitez utiliser afin d'obtenir le meilleur filtre anti-spam.  Veuillez commenter brièvement les raisons de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token_pipeline(words):\n",
    "    cleaning_utils = []\n",
    "    for o in cleaning_utils:\n",
    "        words = o(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque:** la tokenization peut prendre plusieurs minutes, vu la quantité de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [word_tokenize(o) for o in x_train]\n",
    "x_test = [word_tokenize(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [clean_token_pipeline(o) for o in x_train]\n",
    "x_test = [clean_token_pipeline(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [\" \".join(o) for o in x_train]\n",
    "x_test = [\" \".join(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extraction des attributs (*feature extraction*)\n",
    "\n",
    "À cette étape, on transforme les textes des emails en une série d'attributs.  Les méthodes proposées par Scikit-learn permettent de décider quels mots on garde comme attributs, puis les valeurs de chaque attribut pour chaque email.  Résultat : au lieu d'une chaîne de caractères, chaque email est représenté par une suite de nombres (de la même taille pour tous).\n",
    "\n",
    "**Votre mission** : faire des expériences pour choisir entre les deux méthodes suivantes celle qui aboutira aux meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation des textes des emails en vecteurs d'attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_feature(vectorizer, raw_tokenize_data):\n",
    "    return vectorizer.transform(raw_tokenize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = convert_to_feature(vectorizer, x_train)\n",
    "x_test_features  = convert_to_feature(vectorizer, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choix de la méthode de classification et entraînement\n",
    "\n",
    "Dans cette section, vous allez pouvoir choisir le classifieur.  On vous propose de travailler avec un modèle de type *Naïve Bayes*, à choisir parmi plusieurs variantes disponibles dans Scikit-learn, et qui sont décrites à https://scikit-learn.org/stable/modules/naive_bayes.html.\n",
    "\n",
    "Voici à titre d'exemple l'utilisation du classifieur *Naïve Bayes Multinomial*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = nb.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x_train_features.toarray(), y_train) # entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_train_features.toarray(),y_train) # score F1 du modèle sur les données d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Votre mission :** essayer plusieurs classifieurs basés sur l'approche *Naïve Bayes* et choisir celui qui aboutit au meilleur score *sur les données de test*, qui sera calculé dans la section suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Évaluation du classifieur entraîné\n",
    "\n",
    "Dans cette section, vous allez demander au classifieur de classer les données de test (non vues à l'entraînement), et calculer (grâce aux réponses correctes) plusieurs scores qui mesurent les performances de détection de spam.  Veuillez répondre aux questions posées en fin de section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(x_test_features.toarray()) # réponses du système sur les données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_predict)))\n",
    "print(\"Recall:    {:.2f}%\".format(100 * recall_score(y_test, y_predict)))\n",
    "print(\"F1-score:  {:.2f}%\".format(100 * f1_score(y_test, y_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_predict) # affichage élémentaire de la matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage graphique de la matrice de confusion\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_predict, display_labels = ['ham', 'spam']) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Veuillez répondre ici** aux questions suivantes :\n",
    "\n",
    "1. Quel était votre premier score F1 obtenu ?  Quel est votre meilleur score ?  De combien avez-vous progressé grâce à vos choix judicieux ?\n",
    "\n",
    "2. Quels choix de pré-traitement (section 3) vous ont conduit-e à la meilleure solution ?\n",
    "\n",
    "3. Quelle représentation des emails vous a conduit-e à la meilleure solution ?\n",
    "\n",
    "4. Quel classifieur vous a conduit-e à la meilleure solution ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin du Labo 1.**  Veuillez nettoyer ce notebook en gardant seulement les résultats désirés et vos réponses, l'enregistrer, et le soumettre comme devoir sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
